# 压缩和存储

查看Hadoop支持的压缩格式

```shell
[root@node01 native]# hadoop checknative
21/02/05 06:09:10 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native
21/02/05 06:09:10 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/stanlong/hadoop-ha/hadoop-2.9.2/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
snappy:  true /lib64/libsnappy.so.1
zstd  :  false 
lz4:     true revision:10301
bzip2:   true /lib64/libbz2.so.1
openssl: true /lib64/libcrypto.so
```

## 开启Map输出阶段的压缩

开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量

### 参数配置

```properties
# 开启hive中间传输数据压缩功能
set hive.exec.compress.intermediate=true;
# 开启mapreduce中map输出压缩功能
set mapreduce.map.output.compress=true;
# 设置mapreduce中map输出数据的压缩方式
set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
```

### 执行查询

未开启压缩前

```sql
0: jdbc:hive2://node01:10000> select count(ename) name from emp;
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1612469998745_0001
INFO  : The url to track the job: http://node03:8088/proxy/application_1612469998745_0001/
INFO  : Starting Job = job_1612469998745_0001, Tracking URL = http://node03:8088/proxy/application_1612469998745_0001/
INFO  : Kill Command = /opt/stanlong/hadoop-ha/hadoop-2.9.2/bin/hadoop job  -kill job_1612469998745_0001
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
INFO  : 2021-02-05 06:24:07,688 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-05 06:25:09,252 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-05 06:25:29,789 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 24.09 sec
INFO  : 2021-02-05 06:25:43,581 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 27.69 sec
INFO  : MapReduce Total cumulative CPU time: 27 seconds 690 msec
INFO  : Ended Job = job_1612469998745_0001
+-------+--+
| name  |
+-------+--+
| 14    |
+-------+--+
1 row selected (178.528 seconds)
```

开启压缩后

```sql
0: jdbc:hive2://node01:10000> set hive.exec.compress.intermediate;
+---------------------------------------+--+
|                  set                  |
+---------------------------------------+--+
| hive.exec.compress.intermediate=true  |
+---------------------------------------+--+
1 row selected (0.009 seconds)
0: jdbc:hive2://node01:10000> set mapreduce.map.output.compress;
+-------------------------------------+--+
|                 set                 |
+-------------------------------------+--+
| mapreduce.map.output.compress=true  |
+-------------------------------------+--+
1 row selected (0.012 seconds)
0: jdbc:hive2://node01:10000> set mapreduce.map.output.compress.codec;
+--------------------------------------------------------------------------------+--+
|                                      set                                       |
+--------------------------------------------------------------------------------+--+
| mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec  |
+--------------------------------------------------------------------------------+--+
1 row selected (0.01 seconds)

0: jdbc:hive2://node01:10000> select count(ename) name from emp;
INFO  : Number of reduce tasks determined at compile time: 1
INFO  : In order to change the average load for a reducer (in bytes):
INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
INFO  : In order to limit the maximum number of reducers:
INFO  :   set hive.exec.reducers.max=<number>
INFO  : In order to set a constant number of reducers:
INFO  :   set mapreduce.job.reduces=<number>
INFO  : number of splits:2
INFO  : Submitting tokens for job: job_1612469998745_0002
INFO  : The url to track the job: http://node03:8088/proxy/application_1612469998745_0002/
INFO  : Starting Job = job_1612469998745_0002, Tracking URL = http://node03:8088/proxy/application_1612469998745_0002/
INFO  : Kill Command = /opt/stanlong/hadoop-ha/hadoop-2.9.2/bin/hadoop job  -kill job_1612469998745_0002
INFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
INFO  : 2021-02-05 06:28:30,314 Stage-1 map = 0%,  reduce = 0%
INFO  : 2021-02-05 06:28:43,007 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.4 sec
INFO  : 2021-02-05 06:29:41,628 Stage-1 map = 50%,  reduce = 17%, Cumulative CPU 2.4 sec
INFO  : 2021-02-05 06:30:06,063 Stage-1 map = 100%,  reduce = 17%, Cumulative CPU 29.41 sec
INFO  : 2021-02-05 06:30:13,792 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 33.01 sec
INFO  : MapReduce Total cumulative CPU time: 33 seconds 10 msec
INFO  : Ended Job = job_1612469998745_0002
+-------+--+
| name  |
+-------+--+
| 14    |
+-------+--+
1 row selected (127.585 seconds)
```

## 开启Reduce输出阶段压缩

当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性`hive.exec.compress.output`控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。

### 参数配置

```properties
# 开启hive最终输出数据压缩功能
set hive.exec.compress.output=true;
# 开启mapreduce最终输出数据压缩
set mapreduce.output.fileoutputformat.compress=true;
# 设置mapreduce最终数据输出压缩方式
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
# 设置mapreduce最终数据输出压缩为块压缩
set mapreduce.output.fileoutputformat.compress.type=BLOCK;
```

### 执行查询

未开启压缩

```sql
hive> insert overwrite local directory
    >  '/var/data/hive/student' select * from student;
# 查看数据
[root@node02 ~]# cd /var/data/hive/student
[root@node02 student]# ll
total 4
-rw-r--r-- 1 root root 45 Feb  5 07:04 000000_0
[root@node02 student]# cat 000000_0 
1001沈万三
1002朱元璋
1003陆春香
```

开启压缩

```sql
hive> set hive.exec.compress.output;
hive.exec.compress.output=true
hive> set mapreduce.output.fileoutputformat.compress;
mapreduce.output.fileoutputformat.compress=true
hive> set mapreduce.output.fileoutputformat.compress.codec;
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec
hive> set mapreduce.output.fileoutputformat.compress.type;
mapreduce.output.fileoutputformat.compress.type=BLOCK

hive> insert overwrite local directory
    >  '/var/data/hive/student_snappy' select * from student;

# 查看数据
[root@node02 student]# cd /var/data/hive/student_snappy
[root@node02 student_snappy]# ll
total 4
-rw-r--r-- 1 root root 54 Feb  5 07:09 000000_0.snappy
[root@node02 student_snappy]# cat 000000_0.snappy 
-.-p1001沈万三
1002朱元璋,3陆春香
```





